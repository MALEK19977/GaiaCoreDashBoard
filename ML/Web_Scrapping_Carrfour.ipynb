{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://www.carrefour.tn/search.html?query=lait&page=1\n",
      "Scraping page 2: https://www.carrefour.tn/search.html?query=lait&page=2\n",
      "Scraping page 3: https://www.carrefour.tn/search.html?query=lait&page=3\n",
      "Scraping page 4: https://www.carrefour.tn/search.html?query=lait&page=4\n",
      "Scraping page 5: https://www.carrefour.tn/search.html?query=lait&page=5\n",
      "Scraping page 6: https://www.carrefour.tn/search.html?query=lait&page=6\n",
      "Scraping page 7: https://www.carrefour.tn/search.html?query=lait&page=7\n",
      "Data saved to carrefour_products.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Step 1: Define the path to ChromeDriver\n",
    "path = \"C:/Users/21695/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "# Step 2: Initialize the Service object\n",
    "service = Service(path)\n",
    "\n",
    "# Step 3: Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Step 4: Define the base URL and maximum number of pages to scrape\n",
    "base_url = \"https://www.carrefour.tn/search.html?query=lait&page={}\"\n",
    "max_pages = 7  # Adjust this value based on the total number of pages\n",
    "\n",
    "# Step 5: Create an Excel workbook and worksheet\n",
    "workbook = Workbook()\n",
    "sheet = workbook.active\n",
    "sheet.title = \"Products\"\n",
    "sheet.append([\"Page\", \"Brand\", \"Name\", \"Price\"])  # Write header row\n",
    "\n",
    "# Step 6: Scrape products from all pages\n",
    "for page in range(1, max_pages + 1):\n",
    "    url = base_url.format(page)\n",
    "    print(f\"Scraping page {page}: {url}\")\n",
    "\n",
    "    try:\n",
    "        # Open the current page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for product containers to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.searchPage-categoryItem--oB.grid.gap-y-2xs')))\n",
    "\n",
    "        # Find all product containers\n",
    "        products = driver.find_elements(By.CSS_SELECTOR, 'div.searchPage-categoryItem--oB.grid.gap-y-2xs')\n",
    "\n",
    "        # Loop through each product and extract details\n",
    "        for product in products:\n",
    "            try:\n",
    "                # Extract product name\n",
    "                try:\n",
    "                    name_element = product.find_element(By.CSS_SELECTOR, 'span.item-name-LPg')\n",
    "                    name = name_element.text.strip() if name_element else \"N/A\"\n",
    "                except Exception:\n",
    "                    name = \"N/A\"\n",
    "\n",
    "                # Extract product price\n",
    "                try:\n",
    "                    price_element = product.find_element(By.CSS_SELECTOR, 'div.item-prices-EOR.text-colorDefault')\n",
    "                    price = price_element.text.strip() if price_element else \"N/A\"\n",
    "                except Exception:\n",
    "                    price = \"N/A\"\n",
    "\n",
    "                # Extract product brand\n",
    "                try:\n",
    "                    brand_element = product.find_element(By.CSS_SELECTOR, 'div.item-carrefourLabel-AeJ')\n",
    "                    brand = brand_element.text.strip() if brand_element else \"N/A\"\n",
    "                except Exception:\n",
    "                    brand = \"N/A\"\n",
    "\n",
    "                  # Extract product brand\n",
    "                try:\n",
    "                    description_element = product.find_element(By.CSS_SELECTOR, 'div.richContent-root-Ddk')\n",
    "                    description = description_element.text.strip() if description_element else \"N/A\"\n",
    "                except Exception:\n",
    "                    brand = \"N/A\"\n",
    "                # Append the data to the Excel sheet\n",
    "                sheet.append([ name,brand, description,price])\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data for a product: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page {page}: {e}\")\n",
    "\n",
    "# Step 7: Save the Excel file\n",
    "output_file = \"carrefour_products.xlsx\"\n",
    "workbook.save(output_file)\n",
    "print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# Step 8: Close the browser\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
